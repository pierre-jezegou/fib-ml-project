{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION\n",
    "In this notebook, we will implement linear regression and apply it to our dataset. We will use the dataset we previously built and preprocessed.\n",
    "We will use the `scikit-learn` and manual implementation library to implement linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from helpers import plot_variable_importance, write_metrics_in_csv, plot_ypred_vs_yreal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dataset\n",
    "filename: str = 'dataRead_processed.pkl.bz2'\n",
    "dataset = pd.read_pickle(filename, compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed\n",
    "dataset = dataset.drop(columns=[\n",
    "    'city_attraction_area',\n",
    "    'non_scholarized_15_years_old_or_more_2020',\n",
    "    ])\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = dataset.drop('total_passengers_2022', axis=1)\n",
    "y = dataset['total_passengers_2022']\n",
    "\n",
    "# Last preprocessing steps\n",
    "num_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "\n",
    "# Prepare the final preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('cat', OneHotEncoder(), cat_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit learn regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the final pipeline\n",
    "linear_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Separate the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# Write metrics in a csv file\n",
    "mse = mean_squared_error(y_test, y_pred_linear)\n",
    "r2 = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "plot_ypred_vs_yreal(y_pred_linear, y_test, 'Linear regression', False, True)\n",
    "\n",
    "write_metrics_in_csv(y_pred_linear,\n",
    "                     y_test,\n",
    "                     'linear_regression',\n",
    "                     {'tuning': None}\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_importance(linear_model, X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ypred_vs_yreal(y_pred_linear, y_test, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regularization\n",
    "To avoid unwanted too complex model and overfitting, we will use Ridge regularization. Ridge regression adds a penalty term to the cost function. The penalty term is the sum of the squares of the coefficients multiplied by the regularization parameter alpha. The regularization parameter alpha is a hyperparameter that controls the strength of the penalty term. The larger the alpha, the stronger the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "param_grid = {\n",
    "    'regressor__alpha': [10**i for i in range(-3, 3)],\n",
    "    'regressor__fit_intercept': [True, False],\n",
    "    'regressor__tol': [1e-4, 1e-3, 1e-2],\n",
    "    'regressor__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Créer le pipeline complet avec le préprocesseur et le modèle\n",
    "ridge_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Meilleurs hyperparamètres trouvés\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the best hyperparameters\n",
    "ridge_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "# Write metrics in a csv file\n",
    "write_metrics_in_csv(y_pred_ridge, y_test, 'ridge_regression', grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ypred_vs_yreal(y_pred_ridge, y_test, 'Ridge regression', True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_importance(ridge_model, X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "param_grid = {\n",
    "    'regressor__alpha': [10**i for i in range(-3, 3)],  # Réglage de l'alpha pour Lasso\n",
    "    'regressor__fit_intercept': [True, False],          # Option pour inclure ou non l'intercept\n",
    "    'regressor__tol': [1e-4, 1e-3, 1e-2],               # Tolérance pour l'optimisation\n",
    "    'regressor__selection': ['cyclic', 'random']        # Stratégie de sélection pour l'optimisation\n",
    "}\n",
    "\n",
    "# Créer le pipeline complet avec le préprocesseur et le modèle Lasso\n",
    "lasso_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Lasso())\n",
    "])\n",
    "\n",
    "# Utiliser GridSearchCV pour trouver les meilleurs hyperparamètres\n",
    "grid_search = GridSearchCV(estimator=lasso_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Meilleurs hyperparamètres trouvés\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the best hyperparameters\n",
    "lasso_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred_lasso = ridge_model.predict(X_test)\n",
    "\n",
    "# Write metrics in a csv file\n",
    "write_metrics_in_csv(y_pred_ridge, y_test, 'ridge_regression', grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_importance(lasso_model, X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels\n",
    "plt.xlabel('Real values')\n",
    "plt.ylabel('Predicted values')\n",
    "\n",
    "plt.title(f'Linear models - Predicted vs Real values')\n",
    "\n",
    "# Plot\n",
    "sns.scatterplot(x=y_test, y=y_pred_linear, alpha=0.75, marker='+', label='Linear regression')\n",
    "sns.scatterplot(x=y_test, y=y_pred_ridge, alpha=0.75, marker='x', label='Ridge regression')\n",
    "sns.scatterplot(x=y_test, y=y_pred_lasso, alpha=0.75, marker='*', label='Lasso regression')\n",
    "\n",
    "# # x, yin log scale\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "\n",
    "plt.plot([np.min(y_test), np.max(y_test)], [np.min(y_test), np.max(y_test)], '--', color='red')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
